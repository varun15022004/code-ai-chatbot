{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Model Training for Furniture Recommendation Platform\n",
    "\n",
    "This notebook covers:\n",
    "1. NLP Model Training (Text Embeddings)\n",
    "2. Computer Vision Model Training (Image Embeddings)\n",
    "3. Vector Database Setup\n",
    "4. Generative AI Integration\n",
    "5. Model Evaluation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "try:\n",
    "    df = pd.read_csv('../data/cleaned_furniture_data.csv')\n",
    "    print(f\"‚úÖ Loaded cleaned dataset with {len(df)} products\")\n",
    "except FileNotFoundError:\n",
    "    # Fallback to original dataset\n",
    "    print(\"‚ö†Ô∏è Cleaned dataset not found. Loading and cleaning original dataset...\")\n",
    "    df = pd.read_csv('../data/intern_data_ikarus.csv')\n",
    "    \n",
    "    # Quick cleaning\n",
    "    from utils.helpers import safe_parse_list, clean_price, create_combined_text\n",
    "    \n",
    "    df['price_numeric'] = df['price'].apply(clean_price)\n",
    "    df['categories_list'] = df['categories'].apply(safe_parse_list)\n",
    "    df['images_list'] = df['images'].apply(safe_parse_list)\n",
    "    df['combined_text'] = df.apply(create_combined_text, axis=1)\n",
    "    \n",
    "    print(f\"‚úÖ Cleaned {len(df)} products\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Sample products for testing\n",
    "sample_products = df.sample(n=min(100, len(df)), random_state=42)\n",
    "print(f\"\\nüìã Working with {len(sample_products)} sample products for model training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLP Model Training - Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Sentence Transformer model\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "print(f\"Loading embedding model: {model_name}\")\n",
    "\n",
    "# Load pre-trained model\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "print(f\"‚úÖ Model loaded successfully\")\n",
    "print(f\"Model dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Test with sample texts\n",
    "test_texts = [\n",
    "    \"Modern comfortable office chair\",\n",
    "    \"Wooden dining table for 6 people\",\n",
    "    \"Grey sectional sofa with storage\"\n",
    "]\n",
    "\n",
    "test_embeddings = embedding_model.encode(test_texts)\n",
    "print(f\"\\nüß™ Test embeddings shape: {test_embeddings.shape}\")\n",
    "print(f\"Sample embedding (first 10 dimensions): {test_embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for product texts\n",
    "print(\"üîÑ Generating embeddings for product texts...\")\n",
    "\n",
    "# Use combined text for embeddings\n",
    "product_texts = sample_products['combined_text'].fillna('').tolist()\n",
    "\n",
    "# Generate embeddings in batches to avoid memory issues\n",
    "batch_size = 32\n",
    "all_embeddings = []\n",
    "\n",
    "for i in range(0, len(product_texts), batch_size):\n",
    "    batch_texts = product_texts[i:i + batch_size]\n",
    "    batch_embeddings = embedding_model.encode(batch_texts, convert_to_numpy=True)\n",
    "    all_embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    if (i + batch_size) % 100 == 0 or i + batch_size >= len(product_texts):\n",
    "        print(f\"Processed {min(i + batch_size, len(product_texts))}/{len(product_texts)} products\")\n",
    "\n",
    "product_embeddings = np.array(all_embeddings)\n",
    "print(f\"\\n‚úÖ Generated embeddings for {len(product_embeddings)} products\")\n",
    "print(f\"Embeddings shape: {product_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic search functionality\n",
    "def semantic_search(query, embeddings, texts, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform semantic search using embeddings\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
    "    \n",
    "    # Get top results\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'index': idx,\n",
    "            'similarity': similarities[idx],\n",
    "            'text': texts[idx][:200] + '...' if len(texts[idx]) > 200 else texts[idx]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test semantic search\n",
    "test_queries = [\n",
    "    \"comfortable office chair\",\n",
    "    \"wooden dining furniture\",\n",
    "    \"storage solutions for bedroom\"\n",
    "]\n",
    "\n",
    "print(\"üîç Testing Semantic Search:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    results = semantic_search(query, product_embeddings, product_texts, top_k=3)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. Similarity: {result['similarity']:.3f}\")\n",
    "        print(f\"   Text: {result['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Computer Vision Model - Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import computer vision libraries\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Initialize pre-trained ResNet model for image feature extraction\n",
    "print(\"üñºÔ∏è Loading computer vision model...\")\n",
    "\n",
    "# Load pre-trained ResNet50\n",
    "resnet_model = models.resnet50(pretrained=True)\n",
    "resnet_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Remove the final classification layer to get feature vectors\n",
    "feature_extractor = torch.nn.Sequential(*list(resnet_model.children())[:-1])\n",
    "\n",
    "# Image preprocessing pipeline\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Computer vision model loaded successfully\")\n",
    "\n",
    "def extract_image_features(image_url):\n",
    "    \"\"\"\n",
    "    Extract features from an image URL\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Download image\n",
    "        response = requests.get(image_url, timeout=10)\n",
    "        image = Image.open(io.BytesIO(response.content)).convert('RGB')\n",
    "        \n",
    "        # Preprocess image\n",
    "        input_tensor = preprocess(image).unsqueeze(0)\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            features = feature_extractor(input_tensor)\n",
    "            features = features.squeeze().numpy()\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Test image feature extraction with a few sample images\n",
    "print(\"\\nüß™ Testing image feature extraction...\")\n",
    "\n",
    "# Get products with valid image URLs\n",
    "products_with_images = sample_products[sample_products['images_list'].apply(\n",
    "    lambda x: isinstance(x, list) and len(x) > 0\n",
    ")].head(5)\n",
    "\n",
    "image_features = []\n",
    "valid_images = []\n",
    "\n",
    "for idx, product in products_with_images.iterrows():\n",
    "    if isinstance(product['images_list'], list) and len(product['images_list']) > 0:\n",
    "        image_url = product['images_list'][0].strip()\n",
    "        \n",
    "        if image_url.startswith('http'):\n",
    "            print(f\"Processing: {product['title'][:50]}...\")\n",
    "            features = extract_image_features(image_url)\n",
    "            \n",
    "            if features is not None:\n",
    "                image_features.append(features)\n",
    "                valid_images.append({\n",
    "                    'title': product['title'],\n",
    "                    'url': image_url,\n",
    "                    'features': features\n",
    "                })\n",
    "\n",
    "if image_features:\n",
    "    image_features_array = np.array(image_features)\n",
    "    print(f\"\\n‚úÖ Extracted features from {len(image_features)} images\")\n",
    "    print(f\"Feature vector shape: {image_features_array.shape}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No valid images found for feature extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generative AI - Product Description Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Generative AI model for product descriptions\n",
    "print(\"üé® Loading Generative AI model...\")\n",
    "\n",
    "genai_model_name = 'google/flan-t5-small'  # Lightweight model for testing\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(genai_model_name)\n",
    "    genai_model = AutoModelForSeq2SeqLM.from_pretrained(genai_model_name)\n",
    "    print(f\"‚úÖ Generative AI model loaded: {genai_model_name}\")\n",
    "    genai_available = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load GenAI model: {str(e)}\")\n",
    "    print(\"Will use template-based descriptions instead\")\n",
    "    genai_available = False\n",
    "\n",
    "def generate_product_description(title, category=\"\", material=\"\", color=\"\"):\n",
    "    \"\"\"\n",
    "    Generate creative product description\n",
    "    \"\"\"\n",
    "    if genai_available:\n",
    "        # Create prompt\n",
    "        prompt_parts = [\n",
    "            \"Write a creative, engaging product description for this furniture:\",\n",
    "            f\"Product: {title}\"\n",
    "        ]\n",
    "        \n",
    "        if category:\n",
    "            prompt_parts.append(f\"Category: {category}\")\n",
    "        if material:\n",
    "            prompt_parts.append(f\"Material: {material}\")\n",
    "        if color:\n",
    "            prompt_parts.append(f\"Color: {color}\")\n",
    "        \n",
    "        prompt = \" \".join(prompt_parts)\n",
    "        \n",
    "        try:\n",
    "            # Generate description\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=256, truncation=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = genai_model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_length=80,\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            description = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return description.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating description: {e}\")\n",
    "    \n",
    "    # Fallback: template-based description\n",
    "    templates = [\n",
    "        f\"Discover the perfect blend of style and functionality with this {color} {category}.\",\n",
    "        f\"Transform your space with this beautifully crafted {material} {category}.\",\n",
    "        f\"Experience comfort and elegance with this premium {category} piece.\",\n",
    "        f\"Add sophistication to your home with this {color} {category}.\"\n",
    "    ]\n",
    "    \n",
    "    # Choose template based on available information\n",
    "    if color and category:\n",
    "        return templates[0]\n",
    "    elif material and category:\n",
    "        return templates[1]\n",
    "    elif category:\n",
    "        return templates[2]\n",
    "    else:\n",
    "        return \"Enhance your living space with this thoughtfully designed furniture piece.\"\n",
    "\n",
    "# Test description generation\n",
    "print(\"\\nüß™ Testing product description generation:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_products = [\n",
    "    {\n",
    "        'title': 'Modern Ergonomic Office Chair',\n",
    "        'category': 'Office Furniture',\n",
    "        'material': 'Fabric',\n",
    "        'color': 'Black'\n",
    "    },\n",
    "    {\n",
    "        'title': 'Scandinavian Dining Table',\n",
    "        'category': 'Dining Room',\n",
    "        'material': 'Oak Wood',\n",
    "        'color': 'Natural'\n",
    "    },\n",
    "    {\n",
    "        'title': 'Vintage Leather Armchair',\n",
    "        'category': 'Living Room',\n",
    "        'material': 'Leather',\n",
    "        'color': 'Brown'\n",
    "    }\n",
    "]\n",
    "\n",
    "for product in test_products:\n",
    "    description = generate_product_description(\n",
    "        product['title'],\n",
    "        product['category'],\n",
    "        product['material'],\n",
    "        product['color']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìã Product: {product['title']}\")\n",
    "    print(f\"üé® Generated: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vector Database Setup & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate vector database operations (in-memory for testing)\n",
    "class SimpleVectorDB:\n",
    "    def __init__(self):\n",
    "        self.vectors = {}\n",
    "        self.metadata = {}\n",
    "    \n",
    "    def upsert(self, product_id, vector, metadata):\n",
    "        self.vectors[product_id] = vector\n",
    "        self.metadata[product_id] = metadata\n",
    "    \n",
    "    def query(self, query_vector, top_k=10):\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        similarities = []\n",
    "        for product_id, vector in self.vectors.items():\n",
    "            similarity = cosine_similarity([query_vector], [vector])[0][0]\n",
    "            similarities.append({\n",
    "                'id': product_id,\n",
    "                'score': float(similarity),\n",
    "                'metadata': self.metadata[product_id]\n",
    "            })\n",
    "        \n",
    "        # Sort by similarity score\n",
    "        similarities.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "    \n",
    "    def stats(self):\n",
    "        return {\n",
    "            'total_vectors': len(self.vectors),\n",
    "            'dimension': len(list(self.vectors.values())[0]) if self.vectors else 0\n",
    "        }\n",
    "\n",
    "# Initialize vector database\n",
    "vector_db = SimpleVectorDB()\n",
    "\n",
    "print(\"üóÑÔ∏è Setting up vector database...\")\n",
    "\n",
    "# Add products to vector database\n",
    "for idx, (_, product) in enumerate(sample_products.iterrows()):\n",
    "    product_id = product.get('uniq_id', f'product_{idx}')\n",
    "    \n",
    "    # Use the embedding we generated earlier\n",
    "    if idx < len(product_embeddings):\n",
    "        vector = product_embeddings[idx]\n",
    "        \n",
    "        metadata = {\n",
    "            'title': product.get('title', ''),\n",
    "            'price': product.get('price_numeric'),\n",
    "            'category': product.get('categories_list', [{}])[0] if isinstance(product.get('categories_list'), list) and product.get('categories_list') else 'Unknown',\n",
    "            'material': product.get('material'),\n",
    "            'color': product.get('color'),\n",
    "            'brand': product.get('brand')\n",
    "        }\n",
    "        \n",
    "        vector_db.upsert(product_id, vector, metadata)\n",
    "\n",
    "stats = vector_db.stats()\n",
    "print(f\"‚úÖ Vector database setup complete\")\n",
    "print(f\"üìä Stats: {stats['total_vectors']} products, {stats['dimension']} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complete search pipeline\n",
    "def complete_search_pipeline(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Test the complete search pipeline:\n",
    "    Query -> Embedding -> Vector Search -> Description Generation\n",
    "    \"\"\"\n",
    "    print(f\"üîç Query: '{query}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 1. Generate query embedding\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)[0]\n",
    "    \n",
    "    # 2. Search vector database\n",
    "    results = vector_db.query(query_embedding, top_k=top_k)\n",
    "    \n",
    "    # 3. Generate descriptions and format results\n",
    "    formatted_results = []\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        metadata = result['metadata']\n",
    "        \n",
    "        # Generate AI description\n",
    "        ai_description = generate_product_description(\n",
    "            metadata['title'],\n",
    "            metadata['category'],\n",
    "            metadata['material'],\n",
    "            metadata['color']\n",
    "        )\n",
    "        \n",
    "        formatted_result = {\n",
    "            'rank': i,\n",
    "            'similarity': result['score'],\n",
    "            'title': metadata['title'],\n",
    "            'price': metadata['price'],\n",
    "            'category': metadata['category'],\n",
    "            'material': metadata['material'],\n",
    "            'color': metadata['color'],\n",
    "            'ai_description': ai_description\n",
    "        }\n",
    "        \n",
    "        formatted_results.append(formatted_result)\n",
    "        \n",
    "        # Print result\n",
    "        print(f\"{i}. {metadata['title']}\")\n",
    "        print(f\"   üí∞ Price: ${metadata['price']:.2f}\" if metadata['price'] else \"   üí∞ Price: Not available\")\n",
    "        print(f\"   üìä Similarity: {result['score']:.3f}\")\n",
    "        print(f\"   üé® AI Description: {ai_description}\")\n",
    "        print()\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "# Test with various queries\n",
    "test_queries = [\n",
    "    \"comfortable office chair for work\",\n",
    "    \"dining table for family meals\",\n",
    "    \"storage furniture for bedroom\"\n",
    "]\n",
    "\n",
    "print(\"üöÄ Testing Complete Search Pipeline:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_results = {}\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    results = complete_search_pipeline(query, top_k=3)\n",
    "    all_results[query] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "def evaluate_search_quality(test_cases):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of search results\n",
    "    \"\"\"\n",
    "    print(\"üìà Evaluating Search Quality:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    total_queries = len(test_cases)\n",
    "    relevant_results = 0\n",
    "    total_results = 0\n",
    "    \n",
    "    for query, results in test_cases.items():\n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        \n",
    "        query_relevant = 0\n",
    "        for result in results:\n",
    "            # Simple relevance check based on keyword matching\n",
    "            query_words = set(query.lower().split())\n",
    "            title_words = set(result['title'].lower().split())\n",
    "            category_words = set((result['category'] or '').lower().split())\n",
    "            \n",
    "            # Check if there's overlap\n",
    "            if query_words.intersection(title_words.union(category_words)):\n",
    "                query_relevant += 1\n",
    "            \n",
    "            total_results += 1\n",
    "        \n",
    "        relevant_results += query_relevant\n",
    "        precision = query_relevant / len(results) if results else 0\n",
    "        print(f\"  Precision: {precision:.2f} ({query_relevant}/{len(results)} relevant)\")\n",
    "    \n",
    "    overall_precision = relevant_results / total_results if total_results > 0 else 0\n",
    "    print(f\"\\nüéØ Overall Precision: {overall_precision:.2f}\")\n",
    "    print(f\"üìä Total relevant results: {relevant_results}/{total_results}\")\n",
    "    \n",
    "    return {\n",
    "        'overall_precision': overall_precision,\n",
    "        'total_queries': total_queries,\n",
    "        'relevant_results': relevant_results,\n",
    "        'total_results': total_results\n",
    "    }\n",
    "\n",
    "# Performance timing\n",
    "import time\n",
    "\n",
    "def measure_performance(query, num_runs=5):\n",
    "    \"\"\"\n",
    "    Measure search performance\n",
    "    \"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Simulate full search pipeline\n",
    "        query_embedding = embedding_model.encode([query], convert_to_numpy=True)[0]\n",
    "        results = vector_db.query(query_embedding, top_k=5)\n",
    "        \n",
    "        # Generate description for top result\n",
    "        if results:\n",
    "            top_result = results[0]['metadata']\n",
    "            _ = generate_product_description(\n",
    "                top_result['title'],\n",
    "                top_result['category'],\n",
    "                top_result['material'],\n",
    "                top_result['color']\n",
    "            )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    return {\n",
    "        'avg_time': np.mean(times),\n",
    "        'min_time': np.min(times),\n",
    "        'max_time': np.max(times),\n",
    "        'std_time': np.std(times)\n",
    "    }\n",
    "\n",
    "# Run evaluations\n",
    "print(\"üî¨ Running Model Evaluation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Search quality evaluation\n",
    "quality_metrics = evaluate_search_quality(all_results)\n",
    "\n",
    "# Performance evaluation\n",
    "print(\"\\n‚è±Ô∏è Performance Evaluation:\")\n",
    "perf_metrics = measure_performance(\"comfortable office chair\", num_runs=3)\n",
    "print(f\"Average search time: {perf_metrics['avg_time']:.3f} seconds\")\n",
    "print(f\"Min/Max time: {perf_metrics['min_time']:.3f}s / {perf_metrics['max_time']:.3f}s\")\n",
    "\n",
    "# Model statistics\n",
    "print(\"\\nüìä Model Statistics:\")\n",
    "print(f\"Embedding model dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"Vector database size: {stats['total_vectors']} products\")\n",
    "print(f\"Generative AI model: {'Available' if genai_available else 'Fallback templates'}\")\n",
    "print(f\"Image processing: {'Available' if 'image_features' in locals() and image_features else 'Limited'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Export and Deployment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings and model artifacts for deployment\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "print(\"üíæ Saving model artifacts for deployment...\")\n",
    "\n",
    "# Save product embeddings\n",
    "np.save('../models/product_embeddings.npy', product_embeddings)\n",
    "print(\"‚úÖ Product embeddings saved\")\n",
    "\n",
    "# Save product metadata\n",
    "with open('../models/product_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(sample_products.to_dict('records'), f)\n",
    "print(\"‚úÖ Product metadata saved\")\n",
    "\n",
    "# Save vector database\n",
    "with open('../models/vector_db.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'vectors': vector_db.vectors,\n",
    "        'metadata': vector_db.metadata\n",
    "    }, f)\n",
    "print(\"‚úÖ Vector database saved\")\n",
    "\n",
    "# Save model configuration\n",
    "model_config = {\n",
    "    'embedding_model': model_name,\n",
    "    'genai_model': genai_model_name if genai_available else None,\n",
    "    'embedding_dimension': embedding_model.get_sentence_embedding_dimension(),\n",
    "    'num_products': len(sample_products),\n",
    "    'performance_metrics': {\n",
    "        'precision': quality_metrics['overall_precision'],\n",
    "        'avg_search_time': perf_metrics['avg_time']\n",
    "    },\n",
    "    'created_at': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('../models/model_config.json', 'w') as f:\n",
    "    import json\n",
    "    json.dump(model_config, f, indent=2)\n",
    "print(\"‚úÖ Model configuration saved\")\n",
    "\n",
    "print(f\"\\nüéâ Model training and evaluation complete!\")\n",
    "print(f\"üìÅ Artifacts saved in '../models/' directory\")\n",
    "print(f\"üöÄ Ready for deployment integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Deployment Integration Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate deployment integration code\n",
    "deployment_code = '''\n",
    "# AI Models Integration for FastAPI Backend\n",
    "# Add this code to your AIModelManager class\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "class AIModelManager:\n",
    "    def __init__(self, settings):\n",
    "        self.settings = settings\n",
    "        self.embedding_model = None\n",
    "        self.genai_model = None\n",
    "        self.genai_tokenizer = None\n",
    "        self.vector_db = {}\n",
    "        self.product_metadata = []\n",
    "        \n",
    "    async def load_pretrained_models(self):\n",
    "        \"\"\"Load pre-trained models from training artifacts\"\"\"\n",
    "        \n",
    "        # Load model configuration\n",
    "        with open('models/model_config.json', 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Load embedding model\n",
    "        self.embedding_model = SentenceTransformer(config['embedding_model'])\n",
    "        \n",
    "        # Load GenAI model if available\n",
    "        if config['genai_model']:\n",
    "            self.genai_tokenizer = AutoTokenizer.from_pretrained(config['genai_model'])\n",
    "            self.genai_model = AutoModelForSeq2SeqLM.from_pretrained(config['genai_model'])\n",
    "        \n",
    "        # Load vector database\n",
    "        with open('models/vector_db.pkl', 'rb') as f:\n",
    "            db_data = pickle.load(f)\n",
    "            self.vector_db = db_data\n",
    "        \n",
    "        # Load product metadata\n",
    "        with open('models/product_metadata.pkl', 'rb') as f:\n",
    "            self.product_metadata = pickle.load(f)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.vector_db['vectors'])} product vectors\")\n",
    "        logger.info(f\"Model performance: {config['performance_metrics']}\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "# Usage in main.py:\n",
    "# 1. Replace the embedding generation code with:\n",
    "#    await ai_manager.load_pretrained_models()\n",
    "# \n",
    "# 2. The vector search will work immediately with the loaded data\n",
    "#\n",
    "# 3. All search functionality is ready for production use\n",
    "'''\n",
    "\n",
    "with open('../models/deployment_guide.py', 'w') as f:\n",
    "    f.write(deployment_code)\n",
    "\n",
    "print(\"üìö Deployment Guide Generated:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"1. Copy model artifacts to your backend/models/ directory\")\n",
    "print(\"2. Update AIModelManager with the deployment code\")\n",
    "print(\"3. Call load_pretrained_models() instead of training from scratch\")\n",
    "print(\"4. Your search API will be ready with trained embeddings!\")\n",
    "print(\"\\n‚ú® Training complete and ready for deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}